{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmeansDream/AI_Lab_Work/blob/main/AI_Lab_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic imports of libraries and dataset from Google Drive with unzipping and extraction of training data**"
      ],
      "metadata": {
        "id": "8KZAI2JBphaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "iSveyjyXgCBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA1FCBbMFMhU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Drive mounted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amJgG8H0F9ES"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/drive/MyDrive/EuroSAT/EuroSAT_RGB.zip\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(\"Found dataset\")\n",
        "else:\n",
        "    print(\"File not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUWE5AuAGBTZ"
      },
      "outputs": [],
      "source": [
        "extract_path = \"./data\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Unzipped successfully.\")\n",
        "\n",
        "    base_dir = os.path.join(extract_path, \"2750\")\n",
        "    if not os.path.exists(base_dir):\n",
        "        possible_dirs = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]\n",
        "        if possible_dirs:\n",
        "            base_dir = os.path.join(extract_path, possible_dirs[0])\n",
        "\n",
        "    print(f\"Data directory at: {base_dir}\")\n",
        "else:\n",
        "    print(f\"ERROR: Could not find file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparation of data for machine learning algorithms, involves converting data to tensors, splitting the dataset on training and testing and falttening the images into a vector**"
      ],
      "metadata": {
        "id": "vyVD-680pQVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-ylvXBkHF0L"
      },
      "outputs": [],
      "source": [
        "# Convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=base_dir, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "\n",
        "gen = torch.Generator().manual_seed(393)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    full_dataset,\n",
        "    [train_size, test_size],\n",
        "    generator = gen)\n",
        "\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "print(f\"Training set: {len(train_dataset)}\")\n",
        "print(f\"Test set: {len(test_dataset)}\")\n",
        "print(f\"Classes: {full_dataset.classes}\")\n",
        "\n",
        "\n",
        "# Data flattening function\n",
        "def extract_numpy_data(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    X = images.view(images.size(0), -1).numpy()\n",
        "    y = labels.numpy()\n",
        "    return X, y\n",
        "\n",
        "print(\"Flattening data\")\n",
        "X_train, y_train = extract_numpy_data(train_dataset)\n",
        "X_test, y_test = extract_numpy_data(test_dataset)\n",
        "\n",
        "print(f\"Data ready: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper training and basic evaluation function, and declaration of training subsets, that were reduced to 5000, to improve training time.**"
      ],
      "metadata": {
        "id": "59X-RfaKpGdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "SUBSET_SIZE = 5000\n",
        "\n",
        "X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    train_size=SUBSET_SIZE,\n",
        "    stratify=y_train,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def train_and_evaluate(model, model_name):\n",
        "    print(f\"\\n{model_name} Training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training Time: {train_time:.2f} seconds\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    return train_time, acc"
      ],
      "metadata": {
        "id": "vckpte9Djk7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXjs7y5qZKYh"
      },
      "source": [
        "**First algorithm to try learning the dataset will be Naive Bayes, it is the most basic classification algorithm, here we use default setting for Gaussian Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzTDAM68OGFX"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_time, nb_acc = train_and_evaluate(nb_model, \"Naive Bayes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second algorithms is Logistic Regression with semi standard hyper parameters.**"
      ],
      "metadata": {
        "id": "htTwZ6Xso3iR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0qicAJZJflW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    solver='lbfgs',\n",
        "    max_iter=500,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "log_time, log_acc = train_and_evaluate(log_reg, \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VBlQ0Qr08ER"
      },
      "source": [
        "**Now we start training more sophisticated classifier algorithms, starting with Random Forest, that will see a good amount of improvement in comparison to previous models thanks to it's decision-tree structure and ability to recognize basic patterns in dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV9ttHxt1yDc"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    n_jobs=-1,\n",
        "    random_state=393\n",
        ")\n",
        "\n",
        "rf_time, rf_acc = train_and_evaluate(rf_model, \"Random Forest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eluKbNvWYvrp"
      },
      "source": [
        "**Now we set up the SVM, where we should see drastic improvement in comparison to Naive Bayes and Log. Reg., since SVM can decipher not only colors but also more complex relationships between them. The difference with RF will be minimal.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN81fxDNRH8T"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    cache_size=1000\n",
        ")\n",
        "\n",
        "svm_time, svm_acc = train_and_evaluate(svm_model, \"SVM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbBicp0NacvF"
      },
      "source": [
        "**Then we set up data for uploaded pre trained transformer model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuuKNSi9ak-p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vit_processor = ViTImageProcessor.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\"\n",
        ")\n",
        "\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=vit_processor.image_mean,\n",
        "        std=vit_processor.image_std\n",
        "    )\n",
        "])\n",
        "\n",
        "# Import dataset again using transform parameters\n",
        "vit_dataset = datasets.ImageFolder(\n",
        "    root=base_dir,\n",
        "    transform=vit_transform\n",
        ")\n",
        "\n",
        "# Train/test split\n",
        "train_size = int(0.8 * len(vit_dataset))\n",
        "test_size = len(vit_dataset) - train_size\n",
        "\n",
        "train_dataset_vit, test_dataset_vit = torch.utils.data.random_split(\n",
        "    vit_dataset,\n",
        "    [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(393)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset_vit, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset_vit, batch_size=32, shuffle=False)\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=10\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "print(\"Model ready\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TalZS7i-c-Wi"
      },
      "source": [
        "**Conduct training and evaluation of transformer model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqv1lJloa218"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"ViT Training\")\n",
        "start_time = time.time()\n",
        "model.train()\n",
        "\n",
        "for batch in tqdm(train_loader):\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "vit_train_time = time.time() - start_time\n",
        "print(f\"Training Time: {vit_train_time:.2f} seconds\")\n",
        "\n",
        "print(\"Evaluating\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "vit_acc = correct / total\n",
        "print(f\"\\n Transformer Accuracy: {vit_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIwzkwGXzyl_"
      },
      "source": [
        "**Snippet to save trained models for later reuse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ5IQqONqFBY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/EuroSAT_Models/models\"\n",
        "trans_path = \"/content/drive/MyDrive/EuroSAT_Models/models/transformer\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# Save ViT\n",
        "print(f\"Saving model to {trans_path}...\")\n",
        "model.save_pretrained(trans_path)\n",
        "print(\"Saved ViT.\")\n",
        "\n",
        "# Save Naive Bayes\n",
        "joblib.dump(nb_model, os.path.join(save_path, 'nb_model.pkl'))\n",
        "print(\"Saved Naive Bayes Regression.\")\n",
        "\n",
        "# Save Logistic Regression\n",
        "joblib.dump(log_reg, os.path.join(save_path, 'log_reg_model.pkl'))\n",
        "print(\"Saved Logistic Regression.\")\n",
        "\n",
        "# Save SVM\n",
        "joblib.dump(svm_model, os.path.join(save_path, 'svm_model.pkl'))\n",
        "print(\"Saved SVM.\")\n",
        "\n",
        "# Save Random Forest\n",
        "joblib.dump(rf_model, os.path.join(save_path, 'rf_model.pkl'))\n",
        "print(\"Saved Random Forest.\")\n",
        "\n",
        "print(\"Models saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vC7Zv_Pz2pN"
      },
      "source": [
        "**Snippet for loading trained models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73KDHWveyBE4"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from transformers import ViTForImageClassification\n",
        "\n",
        "\n",
        "path = \"/content/drive/MyDrive/EuroSAT_Models/models\"\n",
        "\n",
        "nb_model = joblib.load(f\"{path}/nb_model.pkl\")\n",
        "log_reg = joblib.load(f\"{path}/log_reg_model.pkl\")\n",
        "svm_model = joblib.load(f\"{path}/svm_model.pkl\")\n",
        "rf_model = joblib.load(f\"{path}/rf_model.pkl\")\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(f\"{path}/transformer\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Models loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxBMwOgrmRgJ"
      },
      "source": [
        "**This code snippet will pick a random image from the original test set and feed it to each of the trained models in their respective formats. Then we will be able to see guesses by each model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLHfgCxomgLP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "idx = random.randint(0, len(test_dataset) - 1)\n",
        "image_tensor, label_idx = test_dataset[idx]\n",
        "true_label = full_dataset.classes[label_idx]\n",
        "\n",
        "# Display\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(image_tensor.permute(1, 2, 0))\n",
        "plt.title(f\"True Label: {true_label}\", color='green', fontsize=14, fontweight='bold')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n Model Predictions for Image {idx}\")\n",
        "\n",
        "# Prepare data\n",
        "img_flat = image_tensor.view(1, -1).numpy()\n",
        "\n",
        "classes = full_dataset.classes\n",
        "\n",
        "# Naive Bayes\n",
        "pred_nb = nb_model.predict(img_flat)[0]\n",
        "print(f\"Naive Bayes: {classes[pred_nb]}\")\n",
        "\n",
        "# Logistic Regression\n",
        "pred_lr = log_reg.predict(img_flat)[0]\n",
        "print(f\"Logistic Regression: {classes[pred_lr]}\")\n",
        "\n",
        "# Random Forest\n",
        "pred_rf = rf_model.predict(img_flat)[0]\n",
        "print(f\"Random Forest: {classes[pred_rf]}\")\n",
        "\n",
        "# SVM\n",
        "pred_svm = svm_model.predict(img_flat)[0]\n",
        "print(f\"SVM (RBF): {classes[pred_svm]}\")\n",
        "\n",
        "# Data for transformer\n",
        "img_vit = vit_transform(to_pil_image(image_tensor)).unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(img_vit)\n",
        "    probs = F.softmax(outputs.logits, dim=-1)\n",
        "    confidence, pred_vit_idx = torch.max(probs, dim=-1)\n",
        "    pred_vit = classes[pred_vit_idx.item()]\n",
        "\n",
        "print(f\"Vision Transformer: {pred_vit} (Confidence: {confidence.item()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This code snippet is conducting evaluation of every trained model and creates a pandas table with all main metrics for model comparison**"
      ],
      "metadata": {
        "id": "3QjbLqScpyS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOG_PN028NEb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "results = []\n",
        "\n",
        "# Func for non transformer evaluation\n",
        "def evaluate(name, model, X, y):\n",
        "    print(f\"Evaluating {name}\")\n",
        "\n",
        "    #Inference Time\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X)\n",
        "    end_time = time.time()\n",
        "\n",
        "    inference_time = end_time - start_time\n",
        "    ips = len(X) / inference_time\n",
        "\n",
        "    # Calculate Metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='weighted')\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"Inference Speed\": int(ips),\n",
        "        \"Predictions\": y_pred,\n",
        "        \"True Labels\": y\n",
        "    }\n",
        "\n",
        "# Evaluate ML\n",
        "results.append(evaluate(\"Naive Bayes\", nb_model, X_test, y_test))\n",
        "results.append(evaluate(\"Logistic Regression\", log_reg, X_test, y_test))\n",
        "results.append(evaluate(\"SVM (RBF)\", svm_model, X_test, y_test))\n",
        "results.append(evaluate(\"Random Forest\", rf_model, X_test, y_test))\n",
        "\n",
        "# Evaluate ViT\n",
        "print(\"Evaluating ViT\")\n",
        "\n",
        "model.eval()\n",
        "y_true_vit = []\n",
        "y_pred_vit = []\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        images, labels = batch\n",
        "        images = images.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        y_true_vit.extend(labels.cpu().numpy())\n",
        "        y_pred_vit.extend(preds.cpu().numpy())\n",
        "\n",
        "end_time = time.time()\n",
        "inference_time = end_time - start_time\n",
        "ips_vit = len(y_true_vit) / inference_time\n",
        "\n",
        "# Calculate Metrics\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true_vit, y_pred_vit, average='weighted')\n",
        "acc = accuracy_score(y_true_vit, y_pred_vit)\n",
        "\n",
        "results.append({\n",
        "    \"Model\": \"Vision Transformer\",\n",
        "    \"Accuracy\": acc,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1-Score\": f1,\n",
        "    \"Inference Speed\": int(ips_vit),\n",
        "    \"Predictions\": np.array(y_pred_vit),\n",
        "    \"True Labels\": np.array(y_true_vit)\n",
        "})\n",
        "\n",
        "# Create table\n",
        "df_results = pd.DataFrame(results)\n",
        "df_display = df_results.drop(columns=[\"Predictions\", \"True Labels\"]).copy()\n",
        "\n",
        "cols = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "df_display[cols] = (df_display[cols] * 100).round(2)\n",
        "\n",
        "print(\"\\n Comparison table\")\n",
        "display(df_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This snippet is creating a confusion matrix for each model using matplotlib library**"
      ],
      "metadata": {
        "id": "kLmtVXz7qSrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot a single confusion matrix\n",
        "def plot_confusion(ax, y_true, y_pred, class_names, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        cmap=\"Greens\",\n",
        "        ax=ax,\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names,\n",
        "        cbar=False\n",
        "    )\n",
        "\n",
        "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
        "    ax.set_xlabel(\"Predicted Label\")\n",
        "    ax.set_ylabel(\"True Label\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "class_names = full_dataset.classes\n",
        "\n",
        "for i, res in enumerate(results):\n",
        "    plot_confusion(\n",
        "        axes[i],\n",
        "        res[\"True Labels\"],\n",
        "        res[\"Predictions\"],\n",
        "        class_names,\n",
        "        f\"{res['Model']}\\nAcc: {res['Accuracy']*100:.1f}%\"\n",
        "    )\n",
        "\n",
        "# Hide unused subplots\n",
        "for j in range(len(results), len(axes)):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xFUL5kdb9_4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtrpFxtfF3tH2yMHoJZ2Js",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}