{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN+qmpkK85yylvMYYdHw8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmeansDream/AI_Lab_Work/blob/main/AI_Lab_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvuij7r0CU--"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA1FCBbMFMhU",
        "outputId": "fc8bd779-fb1e-4f2d-be48-bc1c5c53d28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/EuroSAT/EuroSAT_RGB.zip\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Found dataset\")\n",
        "else:\n",
        "    print(\"File not found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amJgG8H0F9ES",
        "outputId": "38ba3bb4-0baf-418c-e977-cb8490af78b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_path = \"./data\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Unzipped successfully.\")\n",
        "\n",
        "    base_dir = os.path.join(extract_path, \"2750\")\n",
        "    if not os.path.exists(base_dir):\n",
        "        possible_dirs = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]\n",
        "        if possible_dirs:\n",
        "            base_dir = os.path.join(extract_path, possible_dirs[0])\n",
        "\n",
        "    print(f\"Data directory at: {base_dir}\")\n",
        "else:\n",
        "    print(f\"ERROR: Could not find file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUWE5AuAGBTZ",
        "outputId": "c8974671-a14e-4977-b13e-58c6c0e7ec8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped successfully.\n",
            "Data directory at: ./data/EuroSAT_RGB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize to 64x64 to be sure, and convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=base_dir, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "print(f\"Training set: {len(train_dataset)}\")\n",
        "print(f\"Test set: {len(test_dataset)}\")\n",
        "print(f\"Classes: {full_dataset.classes}\")\n",
        "\n",
        "\n",
        "# Data flattening function\n",
        "def extract_numpy_data(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    X = images.view(images.size(0), -1).numpy()\n",
        "    y = labels.numpy()\n",
        "    return X, y\n",
        "\n",
        "print(\"Flattening data\")\n",
        "X_train, y_train = extract_numpy_data(train_dataset)\n",
        "X_test, y_test = extract_numpy_data(test_dataset)\n",
        "\n",
        "print(f\"Data ready: {X_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-ylvXBkHF0L",
        "outputId": "ca168a57-3340-4f72-dcec-48e54546a6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 27000\n",
            "Training set: 21600\n",
            "Test set: 5400\n",
            "Classes: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Flattening data\n",
            "Data ready: (21600, 12288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First algorithm to try learning the dataset will be Naive Bayes, it is the most basic classification algorithm,"
      ],
      "metadata": {
        "id": "HXjs7y5qZKYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "\n",
        "# Reducing set amount to 5000 for faster learning\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"\\n Naive Bayes Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "nb_time = time.time() - start_time\n",
        "print(f\"Training Time: {nb_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation on a full set\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "nb_acc = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes Accuracy: {nb_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzTDAM68OGFX",
        "outputId": "71f61dd2-7a2e-4c6e-e8c6-4e9d1379f933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Naive Bayes (Subset) ---\n",
            "Training Time: 0.36 seconds\n",
            "Naive Bayes Accuracy: 34.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Reducing set amount to 5000 for faster learning\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"\\n Logistic Regression Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "log_reg = LogisticRegression(solver='lbfgs', max_iter=500, n_jobs=-1)\n",
        "log_reg.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "log_reg_time = time.time() - start_time\n",
        "print(f\"Training Time: {log_reg_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation on a full set\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "log_acc = accuracy_score(y_test, y_pred_log)\n",
        "print(f\"Logistic Regression Accuracy: {log_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0qicAJZJflW",
        "outputId": "38e372ac-27e7-4329-87bc-c9ed23780e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Logistic Regression (Subset) ---\n",
            "Training Time: 122.77 seconds\n",
            "Logistic Regression Accuracy: 35.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"Random Forest Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=393)\n",
        "rf_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "rf_time = time.time() - start_time\n",
        "print(f\"Training Time: {rf_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {rf_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV9ttHxt1yDc",
        "outputId": "ac4b14c9-1d2a-4efa-c6ff-cd56c2589745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Training\n",
            "Training Time: 32.24 seconds\n",
            "Random Forest Accuracy: 62.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PjlP55AuZDXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up the SVM, where we should see drastic improvement in comparison to two earlier models, since SVM can decipher not only colors but also more complex relationships between them."
      ],
      "metadata": {
        "id": "eluKbNvWYvrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"\\n SVM Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1.0, cache_size=1000)\n",
        "svm_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "svm_time = time.time() - start_time\n",
        "print(f\"Training Time: {svm_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {svm_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN81fxDNRH8T",
        "outputId": "a39a23e9-a047-48aa-fbce-37f0ee2594d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " SVM Training\n",
            "Training Time: 197.51 seconds\n",
            "SVM Accuracy: 62.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we upload and set up data for pre trained transformer model."
      ],
      "metadata": {
        "id": "GbBicp0NacvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "vit_dataset = datasets.ImageFolder(root=base_dir, transform=vit_transform)\n",
        "\n",
        "train_size = int(0.8 * len(vit_dataset))\n",
        "test_size = len(vit_dataset) - train_size\n",
        "train_dataset_vit, test_dataset_vit = torch.utils.data.random_split(vit_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset_vit, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset_vit, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data prepared\")\n",
        "\n",
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=10\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "print(\"Model ready\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "2b75f22629b74d98b4bf0eab8d1b66f3",
            "508b5428912e4decbe3dac0cdceb591f",
            "131f897b08a442928dbb554992645370",
            "dbc7e7263bde4abaab877079ce77cdbf",
            "9f6598a579194a629e0b40e09e7dc16a",
            "82a3136b65274d3e8ada2e6c8b4bb12c",
            "85a56353cf604f44be31de5256c0bec4",
            "97a88a34563e4f919831b2d68f4eb853",
            "579e68e5d3a74122a64bd533a54dcf79",
            "77426137e1d74387ad0c0997e7771371",
            "2dc04f59692249289f0b690f8943fb85",
            "9025a08bdc80440eab8716ee3af250f4",
            "8206c3cfb1334f8ebc4ddee51d31f366",
            "5c33443d2c7c460e934614a15840f776",
            "4f48537f3f524eb7923eede33552cb10",
            "7a32718b00fc4864b0912c0216ad5c55",
            "9ac9f2b6267c489bab1ae674c6be6364",
            "45a804d9ac4c49ed96008590cf006d8c",
            "2c2645786b7e4530a0cbb7b4524c3ad2",
            "8b3ebbb9d3d34551980a3ba53f6add1b",
            "0be639c4c06544d5b767c6aa6a5113e7",
            "d5f15fd6dcd44210bb081b06e6daa340"
          ]
        },
        "id": "RuuKNSi9ak-p",
        "outputId": "acd1ca17-beb4-408c-e2e5-324f53b6b0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Data prepared\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b75f22629b74d98b4bf0eab8d1b66f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9025a08bdc80440eab8716ee3af250f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct training and evaluation of transformer model."
      ],
      "metadata": {
        "id": "TalZS7i-c-Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"ViT Training\")\n",
        "start_time = time.time()\n",
        "model.train()\n",
        "\n",
        "for batch in tqdm(train_loader):\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "vit_train_time = time.time() - start_time\n",
        "print(f\"Training Time: {vit_train_time:.2f} seconds\")\n",
        "\n",
        "print(\"Evaluating\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "vit_acc = correct / total\n",
        "print(f\"\\n Transformer Accuracy: {vit_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqv1lJloa218",
        "outputId": "1a85d200-48c4-4429-f4b2-9b029c69e063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [10:41<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Time: 641.26 seconds\n",
            "Evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169/169 [01:02<00:00,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Transformer Accuracy: 98.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
