{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN+qmpkK85yylvMYYdHw8o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmeansDream/AI_Lab_Work/blob/main/AI_Lab_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvuij7r0CU--"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "GA1FCBbMFMhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/EuroSAT/EuroSAT_RGB.zip\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Found dataset\")\n",
        "else:\n",
        "    print(\"File not found\")"
      ],
      "metadata": {
        "id": "amJgG8H0F9ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_path = \"./data\"\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Unzipped successfully.\")\n",
        "\n",
        "    base_dir = os.path.join(extract_path, \"2750\")\n",
        "    if not os.path.exists(base_dir):\n",
        "        possible_dirs = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]\n",
        "        if possible_dirs:\n",
        "            base_dir = os.path.join(extract_path, possible_dirs[0])\n",
        "\n",
        "    print(f\"Data directory at: {base_dir}\")\n",
        "else:\n",
        "    print(f\"ERROR: Could not find file\")"
      ],
      "metadata": {
        "id": "KUWE5AuAGBTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize to 64x64 to be sure, and convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=base_dir, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Total images: {len(full_dataset)}\")\n",
        "print(f\"Training set: {len(train_dataset)}\")\n",
        "print(f\"Test set: {len(test_dataset)}\")\n",
        "print(f\"Classes: {full_dataset.classes}\")\n",
        "\n",
        "\n",
        "# Data flattening function\n",
        "def extract_numpy_data(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "    data_iter = iter(loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    X = images.view(images.size(0), -1).numpy()\n",
        "    y = labels.numpy()\n",
        "    return X, y\n",
        "\n",
        "print(\"Flattening data\")\n",
        "X_train, y_train = extract_numpy_data(train_dataset)\n",
        "X_test, y_test = extract_numpy_data(test_dataset)\n",
        "\n",
        "print(f\"Data ready: {X_train.shape}\")"
      ],
      "metadata": {
        "id": "u-ylvXBkHF0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First algorithm to try learning the dataset will be Naive Bayes, it is the most basic classification algorithm,"
      ],
      "metadata": {
        "id": "HXjs7y5qZKYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "\n",
        "# Reducing set amount to 5000 for faster learning\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"\\n Naive Bayes Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "nb_time = time.time() - start_time\n",
        "print(f\"Training Time: {nb_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation on a full set\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "nb_acc = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Naive Bayes Accuracy: {nb_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "FzTDAM68OGFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Reducing set amount to 5000 for faster learning\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "# Logistic Regression\n",
        "print(\"\\n Logistic Regression Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "log_reg = LogisticRegression(solver='lbfgs', max_iter=500, n_jobs=-1)\n",
        "log_reg.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "log_reg_time = time.time() - start_time\n",
        "print(f\"Training Time: {log_reg_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation on a full set\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "log_acc = accuracy_score(y_test, y_pred_log)\n",
        "print(f\"Logistic Regression Accuracy: {log_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "p0qicAJZJflW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"Random Forest Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=393)\n",
        "rf_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "rf_time = time.time() - start_time\n",
        "print(f\"Training Time: {rf_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {rf_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "lV9ttHxt1yDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PjlP55AuZDXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up the SVM, where we should see drastic improvement in comparison to two earlier models, since SVM can decipher not only colors but also more complex relationships between them."
      ],
      "metadata": {
        "id": "eluKbNvWYvrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "subset_size = 5000\n",
        "X_train_sub = X_train[:subset_size]\n",
        "y_train_sub = y_train[:subset_size]\n",
        "\n",
        "print(\"\\n SVM Training\")\n",
        "start_time = time.time()\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C=1.0, cache_size=1000)\n",
        "svm_model.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "svm_time = time.time() - start_time\n",
        "print(f\"Training Time: {svm_time:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"SVM Accuracy: {svm_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "yN81fxDNRH8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we upload and set up data for pre trained transformer model."
      ],
      "metadata": {
        "id": "GbBicp0NacvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "vit_dataset = datasets.ImageFolder(root=base_dir, transform=vit_transform)\n",
        "\n",
        "train_size = int(0.8 * len(vit_dataset))\n",
        "test_size = len(vit_dataset) - train_size\n",
        "train_dataset_vit, test_dataset_vit = torch.utils.data.random_split(vit_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset_vit, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset_vit, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data prepared\")\n",
        "\n",
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=10\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "print(\"Model ready\\n\")"
      ],
      "metadata": {
        "id": "RuuKNSi9ak-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct training and evaluation of transformer model."
      ],
      "metadata": {
        "id": "TalZS7i-c-Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "print(\"ViT Training\")\n",
        "start_time = time.time()\n",
        "model.train()\n",
        "\n",
        "for batch in tqdm(train_loader):\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "vit_train_time = time.time() - start_time\n",
        "print(f\"Training Time: {vit_train_time:.2f} seconds\")\n",
        "\n",
        "print(\"Evaluating\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "vit_acc = correct / total\n",
        "print(f\"\\n Transformer Accuracy: {vit_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Nqv1lJloa218"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}